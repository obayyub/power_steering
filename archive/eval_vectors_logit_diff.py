#!/usr/bin/env python3
"""
Evaluate multiple steering vector types using A/B logit difference.

Compares:
- MELBO vectors
- Power iteration vectors (at sv=0)
- CAA vectors
- Power iteration vectors (at sv=CAA)
- Random baseline
"""

import torch
import json
from transformers import AutoModelForCausalLM, AutoTokenizer
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import argparse


def format_chat(tokenizer, user_message: str) -> str:
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": user_message},
    ]
    return tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True, enable_thinking=False
    )


def load_eval_data(data_path: str = "data/corrigibility_eval.json") -> list[dict]:
    """Load evaluation prompts."""
    with open(data_path) as f:
        data = json.load(f)
    return data.get("survival-instinct", [])


def get_ab_logits(model, tokenizer, prompt: str) -> dict:
    """Get logits for A vs B tokens."""
    formatted = format_chat(tokenizer, prompt)
    inputs = tokenizer(formatted, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model(inputs["input_ids"])
        logits = outputs.logits[0, -1, :]

    token_A = tokenizer.encode("A", add_special_tokens=False)[0]
    token_B = tokenizer.encode("B", add_special_tokens=False)[0]

    logit_A = logits[token_A].item()
    logit_B = logits[token_B].item()

    ab_logits = torch.tensor([logit_A, logit_B])
    ab_probs = torch.softmax(ab_logits, dim=0)

    return {
        "logit_A": logit_A,
        "logit_B": logit_B,
        "diff": logit_A - logit_B,
        "prob_A": ab_probs[0].item(),
        "prob_B": ab_probs[1].item(),
        "prefers": "A" if logit_A > logit_B else "B",
    }


def load_vectors(path: str, key: str = "vectors") -> torch.Tensor:
    """Load vectors from a .pt file."""
    data = torch.load(path, map_location="cpu", weights_only=True)
    if key in data:
        vecs = data[key]
    elif "vector" in data:
        # CAA has single vector
        vecs = data["vector"].unsqueeze(0)
    else:
        raise KeyError(f"Could not find vectors in {path}. Keys: {list(data.keys())}")

    # Ensure shape is [num_vectors, hidden_dim]
    if vecs.dim() == 1:
        vecs = vecs.unsqueeze(0)

    return vecs


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", default="Qwen/Qwen3-0.6B")
    parser.add_argument("--source-layer", type=int, default=7)
    parser.add_argument("--scales", default="-10,-5,0,5,10,20")
    parser.add_argument("--max-questions", type=int, default=100)
    parser.add_argument("--data-path", default="data/corrigibility_eval.json")

    # Vector files
    parser.add_argument("--melbo-vectors", help="Path to MELBO vectors")
    parser.add_argument("--power-iter-vectors", help="Path to power iteration vectors")
    parser.add_argument("--caa-vectors", help="Path to CAA vectors")
    parser.add_argument("--power-iter-caa-vectors", help="Path to power iteration @ CAA vectors")

    args = parser.parse_args()

    scales = [float(s) for s in args.scales.split(",")]

    print(f"Loading {args.model}...")
    tokenizer = AutoTokenizer.from_pretrained(args.model)
    model = AutoModelForCausalLM.from_pretrained(
        args.model, torch_dtype=torch.bfloat16, device_map="auto"
    )
    device = next(model.parameters()).device
    dtype = model.dtype

    # Load all vector sets
    vector_sets = {}

    if args.melbo_vectors:
        print(f"Loading MELBO vectors from {args.melbo_vectors}")
        vector_sets["melbo"] = load_vectors(args.melbo_vectors).to(device=device, dtype=dtype)
        print(f"  Shape: {vector_sets['melbo'].shape}")

    if args.power_iter_vectors:
        print(f"Loading power iter vectors from {args.power_iter_vectors}")
        vector_sets["power_iter"] = load_vectors(args.power_iter_vectors).to(device=device, dtype=dtype)
        print(f"  Shape: {vector_sets['power_iter'].shape}")

    if args.caa_vectors:
        print(f"Loading CAA vectors from {args.caa_vectors}")
        vector_sets["caa"] = load_vectors(args.caa_vectors).to(device=device, dtype=dtype)
        print(f"  Shape: {vector_sets['caa'].shape}")

    if args.power_iter_caa_vectors:
        print(f"Loading power iter @ CAA vectors from {args.power_iter_caa_vectors}")
        vector_sets["power_iter_caa"] = load_vectors(args.power_iter_caa_vectors).to(device=device, dtype=dtype)
        print(f"  Shape: {vector_sets['power_iter_caa'].shape}")

    # Add random baseline
    hidden_dim = model.config.hidden_size
    num_random = 4
    random_vecs = torch.randn(num_random, hidden_dim, device=device, dtype=dtype)
    random_vecs = random_vecs / random_vecs.norm(dim=1, keepdim=True)
    vector_sets["random"] = random_vecs
    print(f"Created {num_random} random baseline vectors")

    # Load eval data
    eval_data = load_eval_data(args.data_path)
    if args.max_questions:
        eval_data = eval_data[:args.max_questions]
    print(f"\nLoaded {len(eval_data)} evaluation questions")

    # Setup steering hook
    down_proj = model.model.layers[args.source_layer].mlp.down_proj
    steering_state = {"vec": None}

    def steering_hook(m, i, o):
        if steering_state["vec"] is not None:
            return o + steering_state["vec"].to(o.device)
        return o

    handle = down_proj.register_forward_hook(steering_hook)

    results = []

    try:
        for q_idx, item in enumerate(eval_data):
            question = item["question"]
            corrigible_letter = item["corrigible_letter"]
            survival_letter = item["survival_letter"]

            # Baseline (no steering)
            steering_state["vec"] = None
            ab = get_ab_logits(model, tokenizer, question)

            results.append({
                "question_idx": q_idx,
                "corrigible_letter": corrigible_letter,
                "survival_letter": survival_letter,
                "vector_type": "baseline",
                "vector_idx": -1,
                "scale": 0.0,
                **ab,
                "chose_survival": ab["prefers"] == survival_letter,
            })

            # Test each vector set
            for vec_type, vectors in vector_sets.items():
                for vec_idx in range(vectors.shape[0]):
                    vec = vectors[vec_idx]

                    # Normalize to unit norm for fair comparison
                    vec_normalized = vec / vec.norm()

                    for scale in scales:
                        if scale == 0.0:
                            continue  # Already tested as baseline

                        steering_state["vec"] = vec_normalized * scale
                        ab = get_ab_logits(model, tokenizer, question)

                        results.append({
                            "question_idx": q_idx,
                            "corrigible_letter": corrigible_letter,
                            "survival_letter": survival_letter,
                            "vector_type": vec_type,
                            "vector_idx": vec_idx,
                            "scale": scale,
                            **ab,
                            "chose_survival": ab["prefers"] == survival_letter,
                        })

            if (q_idx + 1) % 10 == 0:
                print(f"  Processed {q_idx + 1}/{len(eval_data)} questions...")

    finally:
        handle.remove()
        steering_state["vec"] = None

    # Analyze results
    print("\n" + "="*70)
    print("RESULTS SUMMARY")
    print("="*70)

    # Group by vector type and scale
    stats = defaultdict(lambda: {"total": 0, "survival": 0, "logit_diff_sum": 0})

    for r in results:
        key = (r["vector_type"], r["scale"])
        stats[key]["total"] += 1
        if r["chose_survival"]:
            stats[key]["survival"] += 1
        # Compute survival logit diff (positive = prefers survival)
        if r["survival_letter"] == "A":
            stats[key]["logit_diff_sum"] += r["logit_A"] - r["logit_B"]
        else:
            stats[key]["logit_diff_sum"] += r["logit_B"] - r["logit_A"]

    print(f"\n{'Type':<16} {'Scale':<8} {'Survival%':<12} {'Avg Surv Logit Diff':<20} {'N':<6}")
    print("-" * 70)

    # Sort by type then scale
    sorted_keys = sorted(stats.keys(), key=lambda x: (x[0], x[1]))

    for (vtype, scale) in sorted_keys:
        s = stats[(vtype, scale)]
        if s["total"] == 0:
            continue
        surv_pct = 100 * s["survival"] / s["total"]
        avg_logit_diff = s["logit_diff_sum"] / s["total"]
        print(f"{vtype:<16} {scale:<8.1f} {surv_pct:<12.1f} {avg_logit_diff:<20.2f} {s['total']:<6}")

    # Comparison at scale=10
    print("\n" + "-"*70)
    print("Comparison at scale=10.0 (higher survival logit diff = more survival instinct):")
    print("-"*70)

    baseline_stats = stats[("baseline", 0.0)]
    if baseline_stats["total"] > 0:
        base_surv_pct = 100 * baseline_stats["survival"] / baseline_stats["total"]
        base_logit_diff = baseline_stats["logit_diff_sum"] / baseline_stats["total"]
        print(f"  baseline       : {base_surv_pct:5.1f}% survival, avg logit diff = {base_logit_diff:+.2f}")

    for vtype in ["melbo", "power_iter", "caa", "power_iter_caa", "random"]:
        s = stats[(vtype, 10.0)]
        if s["total"] > 0:
            surv_pct = 100 * s["survival"] / s["total"]
            avg_logit_diff = s["logit_diff_sum"] / s["total"]
            delta = avg_logit_diff - base_logit_diff
            print(f"  {vtype:<14} : {surv_pct:5.1f}% survival, avg logit diff = {avg_logit_diff:+.2f} (Î”{delta:+.2f})")

    # Save results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = Path("results") / f"caa_experiment_{timestamp}.json"

    with open(output_file, "w") as f:
        json.dump({
            "model": args.model,
            "source_layer": args.source_layer,
            "scales": scales,
            "vector_files": {
                "melbo": args.melbo_vectors,
                "power_iter": args.power_iter_vectors,
                "caa": args.caa_vectors,
                "power_iter_caa": args.power_iter_caa_vectors,
            },
            "results": results,
        }, f, indent=2)

    print(f"\nSaved detailed results to {output_file}")


if __name__ == "__main__":
    main()
